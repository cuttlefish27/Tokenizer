{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = [\n",
    "    \"this program is an implementation of a tokenizer in python\",\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"the cat lounged peacefully on the windowsill basking in the golden sunlight\",\n",
    "    \"despite the stormy weather they decided to go hiking in the mountains\",\n",
    "    \"she carefully wrapped the gift in bright red paper tying it with a silver ribbon\",\n",
    "    \"as the clock struck midnight the fireworks lit up the dark sky in vibrant colors\",\n",
    "    \"the aroma of freshly baked bread filled the small bakery on the corner of the street\",\n",
    "    \"although the book was long it captivated her so much that she finished it in a single day\",\n",
    "    \"he was surprised to find a handwritten note tucked inside the old library book\",\n",
    "    \"after years of hard work the team finally achieved their dream of winning the championship\",\n",
    "    \"the children giggled as they chased each other around the garden their laughter echoing through the air\",\n",
    "    \"with each passing wave the sandcastle slowly crumbled leaving only a faint outline behind\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "#creates vocabulary for all characters in the input text, removes space since we will be splitting the data into individual words before creating tokenization\n",
    "for index in input_txt:\n",
    "    for char in index:\n",
    "        if char in vocabulary:\n",
    "            pass\n",
    "        else:\n",
    "            vocabulary.append(char)\n",
    "vocabulary.remove(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperates words into a list and creates a frequency dictionary for each word\n",
    "\n",
    "words = []\n",
    "for index in input_txt:\n",
    "    word = index.split()\n",
    "    for word in word:\n",
    "        words.append(word)\n",
    "\n",
    "word_list = {i:words.count(i) for i in set(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mountains': 1,\n",
       " 'despite': 1,\n",
       " 'book': 2,\n",
       " 'giggled': 1,\n",
       " 'freshly': 1,\n",
       " 'dark': 1,\n",
       " 'finally': 1,\n",
       " 'crumbled': 1,\n",
       " 'struck': 1,\n",
       " 'over': 1,\n",
       " 'dog': 1,\n",
       " 'he': 1,\n",
       " 'aroma': 1,\n",
       " 'chased': 1,\n",
       " 'that': 1,\n",
       " 'brown': 1,\n",
       " 'slowly': 1,\n",
       " 'this': 1,\n",
       " 'bright': 1,\n",
       " 'hiking': 1,\n",
       " 'much': 1,\n",
       " 'vibrant': 1,\n",
       " 'after': 1,\n",
       " 'each': 2,\n",
       " 'stormy': 1,\n",
       " 'bakery': 1,\n",
       " 'quick': 1,\n",
       " 'dream': 1,\n",
       " 'faint': 1,\n",
       " 'children': 1,\n",
       " 'tucked': 1,\n",
       " 'inside': 1,\n",
       " 'years': 1,\n",
       " 'weather': 1,\n",
       " 'day': 1,\n",
       " 'lounged': 1,\n",
       " 'baked': 1,\n",
       " 'windowsill': 1,\n",
       " 'midnight': 1,\n",
       " 'find': 1,\n",
       " 'implementation': 1,\n",
       " 'as': 2,\n",
       " 'gift': 1,\n",
       " 'carefully': 1,\n",
       " 'fox': 1,\n",
       " 'passing': 1,\n",
       " 'library': 1,\n",
       " 'sandcastle': 1,\n",
       " 'although': 1,\n",
       " 'sky': 1,\n",
       " 'their': 2,\n",
       " 'note': 1,\n",
       " 'the': 23,\n",
       " 'she': 2,\n",
       " 'ribbon': 1,\n",
       " 'silver': 1,\n",
       " 'small': 1,\n",
       " 'lazy': 1,\n",
       " 'long': 1,\n",
       " 'sunlight': 1,\n",
       " 'achieved': 1,\n",
       " 'surprised': 1,\n",
       " 'handwritten': 1,\n",
       " 'garden': 1,\n",
       " 'program': 1,\n",
       " 'clock': 1,\n",
       " 'behind': 1,\n",
       " 'championship': 1,\n",
       " 'a': 5,\n",
       " 'work': 1,\n",
       " 'basking': 1,\n",
       " 'old': 1,\n",
       " 'through': 1,\n",
       " 'her': 1,\n",
       " 'around': 1,\n",
       " 'lit': 1,\n",
       " 'wrapped': 1,\n",
       " 'echoing': 1,\n",
       " 'bread': 1,\n",
       " 'corner': 1,\n",
       " 'golden': 1,\n",
       " 'leaving': 1,\n",
       " 'tokenizer': 1,\n",
       " 'fireworks': 1,\n",
       " 'to': 2,\n",
       " 'was': 2,\n",
       " 'they': 2,\n",
       " 'up': 1,\n",
       " 'decided': 1,\n",
       " 'only': 1,\n",
       " 'air': 1,\n",
       " 'colors': 1,\n",
       " 'team': 1,\n",
       " 'cat': 1,\n",
       " 'street': 1,\n",
       " 'with': 2,\n",
       " 'so': 1,\n",
       " 'captivated': 1,\n",
       " 'finished': 1,\n",
       " 'winning': 1,\n",
       " 'hard': 1,\n",
       " 'of': 5,\n",
       " 'paper': 1,\n",
       " 'it': 3,\n",
       " 'is': 1,\n",
       " 'jumps': 1,\n",
       " 'python': 1,\n",
       " 'filled': 1,\n",
       " 'in': 6,\n",
       " 'on': 2,\n",
       " 'outline': 1,\n",
       " 'an': 1,\n",
       " 'tying': 1,\n",
       " 'single': 1,\n",
       " 'red': 1,\n",
       " 'wave': 1,\n",
       " 'laughter': 1,\n",
       " 'peacefully': 1,\n",
       " 'go': 1,\n",
       " 'other': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mountains': ['m', 'o', 'u', 'n', 't', 'a', 'i', 'n', 's'],\n",
       " 'despite': ['d', 'e', 's', 'p', 'i', 't', 'e'],\n",
       " 'book': ['b', 'o', 'o', 'k'],\n",
       " 'giggled': ['g', 'i', 'g', 'g', 'l', 'e', 'd'],\n",
       " 'freshly': ['f', 'r', 'e', 's', 'h', 'l', 'y'],\n",
       " 'dark': ['d', 'a', 'r', 'k'],\n",
       " 'finally': ['f', 'i', 'n', 'a', 'l', 'l', 'y'],\n",
       " 'crumbled': ['c', 'r', 'u', 'm', 'b', 'l', 'e', 'd'],\n",
       " 'struck': ['s', 't', 'r', 'u', 'c', 'k'],\n",
       " 'over': ['o', 'v', 'e', 'r'],\n",
       " 'dog': ['d', 'o', 'g'],\n",
       " 'he': ['h', 'e'],\n",
       " 'aroma': ['a', 'r', 'o', 'm', 'a'],\n",
       " 'chased': ['c', 'h', 'a', 's', 'e', 'd'],\n",
       " 'that': ['t', 'h', 'a', 't'],\n",
       " 'brown': ['b', 'r', 'o', 'w', 'n'],\n",
       " 'slowly': ['s', 'l', 'o', 'w', 'l', 'y'],\n",
       " 'this': ['t', 'h', 'i', 's'],\n",
       " 'bright': ['b', 'r', 'i', 'g', 'h', 't'],\n",
       " 'hiking': ['h', 'i', 'k', 'i', 'n', 'g'],\n",
       " 'much': ['m', 'u', 'c', 'h'],\n",
       " 'vibrant': ['v', 'i', 'b', 'r', 'a', 'n', 't'],\n",
       " 'after': ['a', 'f', 't', 'e', 'r'],\n",
       " 'each': ['e', 'a', 'c', 'h'],\n",
       " 'stormy': ['s', 't', 'o', 'r', 'm', 'y'],\n",
       " 'bakery': ['b', 'a', 'k', 'e', 'r', 'y'],\n",
       " 'quick': ['q', 'u', 'i', 'c', 'k'],\n",
       " 'dream': ['d', 'r', 'e', 'a', 'm'],\n",
       " 'faint': ['f', 'a', 'i', 'n', 't'],\n",
       " 'children': ['c', 'h', 'i', 'l', 'd', 'r', 'e', 'n'],\n",
       " 'tucked': ['t', 'u', 'c', 'k', 'e', 'd'],\n",
       " 'inside': ['i', 'n', 's', 'i', 'd', 'e'],\n",
       " 'years': ['y', 'e', 'a', 'r', 's'],\n",
       " 'weather': ['w', 'e', 'a', 't', 'h', 'e', 'r'],\n",
       " 'day': ['d', 'a', 'y'],\n",
       " 'lounged': ['l', 'o', 'u', 'n', 'g', 'e', 'd'],\n",
       " 'baked': ['b', 'a', 'k', 'e', 'd'],\n",
       " 'windowsill': ['w', 'i', 'n', 'd', 'o', 'w', 's', 'i', 'l', 'l'],\n",
       " 'midnight': ['m', 'i', 'd', 'n', 'i', 'g', 'h', 't'],\n",
       " 'find': ['f', 'i', 'n', 'd'],\n",
       " 'implementation': ['i',\n",
       "  'm',\n",
       "  'p',\n",
       "  'l',\n",
       "  'e',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n'],\n",
       " 'as': ['a', 's'],\n",
       " 'gift': ['g', 'i', 'f', 't'],\n",
       " 'carefully': ['c', 'a', 'r', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " 'fox': ['f', 'o', 'x'],\n",
       " 'passing': ['p', 'a', 's', 's', 'i', 'n', 'g'],\n",
       " 'library': ['l', 'i', 'b', 'r', 'a', 'r', 'y'],\n",
       " 'sandcastle': ['s', 'a', 'n', 'd', 'c', 'a', 's', 't', 'l', 'e'],\n",
       " 'although': ['a', 'l', 't', 'h', 'o', 'u', 'g', 'h'],\n",
       " 'sky': ['s', 'k', 'y'],\n",
       " 'their': ['t', 'h', 'e', 'i', 'r'],\n",
       " 'note': ['n', 'o', 't', 'e'],\n",
       " 'the': ['t', 'h', 'e'],\n",
       " 'she': ['s', 'h', 'e'],\n",
       " 'ribbon': ['r', 'i', 'b', 'b', 'o', 'n'],\n",
       " 'silver': ['s', 'i', 'l', 'v', 'e', 'r'],\n",
       " 'small': ['s', 'm', 'a', 'l', 'l'],\n",
       " 'lazy': ['l', 'a', 'z', 'y'],\n",
       " 'long': ['l', 'o', 'n', 'g'],\n",
       " 'sunlight': ['s', 'u', 'n', 'l', 'i', 'g', 'h', 't'],\n",
       " 'achieved': ['a', 'c', 'h', 'i', 'e', 'v', 'e', 'd'],\n",
       " 'surprised': ['s', 'u', 'r', 'p', 'r', 'i', 's', 'e', 'd'],\n",
       " 'handwritten': ['h', 'a', 'n', 'd', 'w', 'r', 'i', 't', 't', 'e', 'n'],\n",
       " 'garden': ['g', 'a', 'r', 'd', 'e', 'n'],\n",
       " 'program': ['p', 'r', 'o', 'g', 'r', 'a', 'm'],\n",
       " 'clock': ['c', 'l', 'o', 'c', 'k'],\n",
       " 'behind': ['b', 'e', 'h', 'i', 'n', 'd'],\n",
       " 'championship': ['c', 'h', 'a', 'm', 'p', 'i', 'o', 'n', 's', 'h', 'i', 'p'],\n",
       " 'a': ['a'],\n",
       " 'work': ['w', 'o', 'r', 'k'],\n",
       " 'basking': ['b', 'a', 's', 'k', 'i', 'n', 'g'],\n",
       " 'old': ['o', 'l', 'd'],\n",
       " 'through': ['t', 'h', 'r', 'o', 'u', 'g', 'h'],\n",
       " 'her': ['h', 'e', 'r'],\n",
       " 'around': ['a', 'r', 'o', 'u', 'n', 'd'],\n",
       " 'lit': ['l', 'i', 't'],\n",
       " 'wrapped': ['w', 'r', 'a', 'p', 'p', 'e', 'd'],\n",
       " 'echoing': ['e', 'c', 'h', 'o', 'i', 'n', 'g'],\n",
       " 'bread': ['b', 'r', 'e', 'a', 'd'],\n",
       " 'corner': ['c', 'o', 'r', 'n', 'e', 'r'],\n",
       " 'golden': ['g', 'o', 'l', 'd', 'e', 'n'],\n",
       " 'leaving': ['l', 'e', 'a', 'v', 'i', 'n', 'g'],\n",
       " 'tokenizer': ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " 'fireworks': ['f', 'i', 'r', 'e', 'w', 'o', 'r', 'k', 's'],\n",
       " 'to': ['t', 'o'],\n",
       " 'was': ['w', 'a', 's'],\n",
       " 'they': ['t', 'h', 'e', 'y'],\n",
       " 'up': ['u', 'p'],\n",
       " 'decided': ['d', 'e', 'c', 'i', 'd', 'e', 'd'],\n",
       " 'only': ['o', 'n', 'l', 'y'],\n",
       " 'air': ['a', 'i', 'r'],\n",
       " 'colors': ['c', 'o', 'l', 'o', 'r', 's'],\n",
       " 'team': ['t', 'e', 'a', 'm'],\n",
       " 'cat': ['c', 'a', 't'],\n",
       " 'street': ['s', 't', 'r', 'e', 'e', 't'],\n",
       " 'with': ['w', 'i', 't', 'h'],\n",
       " 'so': ['s', 'o'],\n",
       " 'captivated': ['c', 'a', 'p', 't', 'i', 'v', 'a', 't', 'e', 'd'],\n",
       " 'finished': ['f', 'i', 'n', 'i', 's', 'h', 'e', 'd'],\n",
       " 'winning': ['w', 'i', 'n', 'n', 'i', 'n', 'g'],\n",
       " 'hard': ['h', 'a', 'r', 'd'],\n",
       " 'of': ['o', 'f'],\n",
       " 'paper': ['p', 'a', 'p', 'e', 'r'],\n",
       " 'it': ['i', 't'],\n",
       " 'is': ['i', 's'],\n",
       " 'jumps': ['j', 'u', 'm', 'p', 's'],\n",
       " 'python': ['p', 'y', 't', 'h', 'o', 'n'],\n",
       " 'filled': ['f', 'i', 'l', 'l', 'e', 'd'],\n",
       " 'in': ['i', 'n'],\n",
       " 'on': ['o', 'n'],\n",
       " 'outline': ['o', 'u', 't', 'l', 'i', 'n', 'e'],\n",
       " 'an': ['a', 'n'],\n",
       " 'tying': ['t', 'y', 'i', 'n', 'g'],\n",
       " 'single': ['s', 'i', 'n', 'g', 'l', 'e'],\n",
       " 'red': ['r', 'e', 'd'],\n",
       " 'wave': ['w', 'a', 'v', 'e'],\n",
       " 'laughter': ['l', 'a', 'u', 'g', 'h', 't', 'e', 'r'],\n",
       " 'peacefully': ['p', 'e', 'a', 'c', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " 'go': ['g', 'o'],\n",
       " 'other': ['o', 't', 'h', 'e', 'r']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_list.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pairs(splits):\n",
    "#creates an instance of a dictionary that has the number of each pair of \n",
    "    pair_counts ={}\n",
    "    for word, freq in word_list.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            if pair_counts.get(pair) is None:\n",
    "                pair_counts.update({pair: 0 })\n",
    "            pair_counts[pair] = pair_counts[pair] + 1\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = count_pairs(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_pair(pair_counts):\n",
    "    most_freq = 0\n",
    "    pair_freq = ()\n",
    "    for pair,freq in pair_counts.items():\n",
    "        if freq > most_freq:\n",
    "            pair_freq = pair\n",
    "    return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_pair = find_most_frequent_pair(pair_counts)\n",
    "merge_rules = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token(a,b, splits):\n",
    "    merge_rules.append('a'+'b')\n",
    "    for word in word_list:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i+1] == b:\n",
    "                split = split[:i] + [a+b] + split[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = merge_token(most_common_pair[0],most_common_pair[1], splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c', 'e')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
