{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = [\n",
    "    \"ab cd ef ab cd\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "#creates vocabulary for all characters in the input text, removes space since we will be splitting the data into individual words before creating tokenization\n",
    "for index in input_txt:\n",
    "    for char in index:\n",
    "        if char in vocabulary:\n",
    "            pass\n",
    "        else:\n",
    "            vocabulary.append(char)\n",
    "vocabulary.remove(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperates words into a list and creates a frequency dictionary for each word\n",
    "\n",
    "words = []\n",
    "for index in input_txt:\n",
    "    word = index.split()\n",
    "    for word in word:\n",
    "        words.append(word)\n",
    "\n",
    "word_list = {i:words.count(i) for i in set(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab': 2, 'cd': 2, 'ef': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab': ['a', 'b'], 'cd': ['c', 'd'], 'ef': ['e', 'f']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_list.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pairs(splits):\n",
    "#creates an instance of a dictionary that has the number of each pair of \n",
    "    pair_counts ={}\n",
    "    for word, freq in word_list.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair = (split[i], split[i+1])\n",
    "            if pair_counts.get(pair) is None:\n",
    "                pair_counts.update({pair: 0 })\n",
    "            pair_counts[pair] = pair_counts[pair] + 1\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b'): 1, ('c', 'd'): 1, ('e', 'f'): 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_counts = count_pairs(splits)\n",
    "\n",
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_pair(pair_counts):\n",
    "    most_freq = 0\n",
    "    pair_freq = ()\n",
    "    for pair, freq in pair_counts.items():\n",
    "        if freq > most_freq:\n",
    "            most_freq = freq\n",
    "            pair_freq = pair\n",
    "    return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 'n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_pair = find_most_frequent_pair(pair_counts)\n",
    "merge_rules = []\n",
    "\n",
    "most_common_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token(a,b, splits):\n",
    "    merge_rules.append(a+b)\n",
    "    for word in word_list:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i+1] == b:\n",
    "                split = split[:i] + [a+b] + split[i+2:]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = merge_token(most_common_pair[0],most_common_pair[1], splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'implementation': ['i',\n",
       "  'm',\n",
       "  'p',\n",
       "  'l',\n",
       "  'e',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n'],\n",
       " 'silver': ['s', 'i', 'l', 'v', 'e', 'r'],\n",
       " 'is': ['i', 's'],\n",
       " 'slowly': ['s', 'l', 'o', 'w', 'l', 'y'],\n",
       " 'peacefully': ['p', 'e', 'a', 'c', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " 'quick': ['q', 'u', 'i', 'c', 'k'],\n",
       " 'children': ['c', 'h', 'i', 'l', 'd', 'r', 'e', 'n'],\n",
       " 'air': ['a', 'i', 'r'],\n",
       " 'laughter': ['l', 'a', 'u', 'g', 'h', 't', 'e', 'r'],\n",
       " 'library': ['l', 'i', 'b', 'r', 'a', 'r', 'y'],\n",
       " 'years': ['y', 'e', 'a', 'r', 's'],\n",
       " 'after': ['a', 'f', 't', 'e', 'r'],\n",
       " 'each': ['e', 'a', 'c', 'h'],\n",
       " 'stormy': ['s', 't', 'o', 'r', 'm', 'y'],\n",
       " 'lounged': ['l', 'o', 'u', 'n', 'g', 'e', 'd'],\n",
       " 'dream': ['d', 'r', 'e', 'a', 'm'],\n",
       " 'freshly': ['f', 'r', 'e', 's', 'h', 'l', 'y'],\n",
       " 'they': ['t', 'h', 'e', 'y'],\n",
       " 'weather': ['w', 'e', 'a', 't', 'h', 'e', 'r'],\n",
       " 'surprised': ['s', 'u', 'r', 'p', 'r', 'i', 's', 'e', 'd'],\n",
       " 'aroma': ['a', 'r', 'o', 'm', 'a'],\n",
       " 'finally': ['f', 'in', 'a', 'l', 'l', 'y'],\n",
       " 'red': ['r', 'e', 'd'],\n",
       " 'with': ['w', 'i', 't', 'h'],\n",
       " 'crumbled': ['c', 'r', 'u', 'm', 'b', 'l', 'e', 'd'],\n",
       " 'much': ['m', 'u', 'c', 'h'],\n",
       " 'handwritten': ['h', 'a', 'n', 'd', 'w', 'r', 'i', 't', 't', 'e', 'n'],\n",
       " 'midnight': ['m', 'i', 'd', 'n', 'i', 'g', 'h', 't'],\n",
       " 'corner': ['c', 'o', 'r', 'n', 'e', 'r'],\n",
       " 'despite': ['d', 'e', 's', 'p', 'i', 't', 'e'],\n",
       " 'garden': ['g', 'a', 'r', 'd', 'e', 'n'],\n",
       " 'chased': ['c', 'h', 'a', 's', 'e', 'd'],\n",
       " 'clock': ['c', 'l', 'o', 'c', 'k'],\n",
       " 'book': ['b', 'o', 'o', 'k'],\n",
       " 'struck': ['s', 't', 'r', 'u', 'c', 'k'],\n",
       " 'only': ['o', 'n', 'l', 'y'],\n",
       " 'jumps': ['j', 'u', 'm', 'p', 's'],\n",
       " 'work': ['w', 'o', 'r', 'k'],\n",
       " 'bread': ['b', 'r', 'e', 'a', 'd'],\n",
       " 'long': ['l', 'o', 'n', 'g'],\n",
       " 'giggled': ['g', 'i', 'g', 'g', 'l', 'e', 'd'],\n",
       " 'through': ['t', 'h', 'r', 'o', 'u', 'g', 'h'],\n",
       " 'as': ['a', 's'],\n",
       " 'single': ['s', 'in', 'g', 'l', 'e'],\n",
       " 'so': ['s', 'o'],\n",
       " 'tokenizer': ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " 'colors': ['c', 'o', 'l', 'o', 'r', 's'],\n",
       " 'tucked': ['t', 'u', 'c', 'k', 'e', 'd'],\n",
       " 'winning': ['w', 'in', 'n', 'in', 'g'],\n",
       " 'carefully': ['c', 'a', 'r', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " 'the': ['t', 'h', 'e'],\n",
       " 'faint': ['f', 'a', 'in', 't'],\n",
       " 'outline': ['o', 'u', 't', 'l', 'in', 'e'],\n",
       " 'this': ['t', 'h', 'i', 's'],\n",
       " 'their': ['t', 'h', 'e', 'i', 'r'],\n",
       " 'note': ['n', 'o', 't', 'e'],\n",
       " 'small': ['s', 'm', 'a', 'l', 'l'],\n",
       " 'leaving': ['l', 'e', 'a', 'v', 'in', 'g'],\n",
       " 'program': ['p', 'r', 'o', 'g', 'r', 'a', 'm'],\n",
       " 'team': ['t', 'e', 'a', 'm'],\n",
       " 'up': ['u', 'p'],\n",
       " 'echoing': ['e', 'c', 'h', 'o', 'in', 'g'],\n",
       " 'it': ['i', 't'],\n",
       " 'to': ['t', 'o'],\n",
       " 'on': ['o', 'n'],\n",
       " 'over': ['o', 'v', 'e', 'r'],\n",
       " 'although': ['a', 'l', 't', 'h', 'o', 'u', 'g', 'h'],\n",
       " 'gift': ['g', 'i', 'f', 't'],\n",
       " 'old': ['o', 'l', 'd'],\n",
       " 'lit': ['l', 'i', 't'],\n",
       " 'achieved': ['a', 'c', 'h', 'i', 'e', 'v', 'e', 'd'],\n",
       " 'sky': ['s', 'k', 'y'],\n",
       " 'a': ['a'],\n",
       " 'sunlight': ['s', 'u', 'n', 'l', 'i', 'g', 'h', 't'],\n",
       " 'baked': ['b', 'a', 'k', 'e', 'd'],\n",
       " 'that': ['t', 'h', 'a', 't'],\n",
       " 'championship': ['c', 'h', 'a', 'm', 'p', 'i', 'o', 'n', 's', 'h', 'i', 'p'],\n",
       " 'python': ['p', 'y', 't', 'h', 'o', 'n'],\n",
       " 'of': ['o', 'f'],\n",
       " 'an': ['a', 'n'],\n",
       " 'captivated': ['c', 'a', 'p', 't', 'i', 'v', 'a', 't', 'e', 'd'],\n",
       " 'golden': ['g', 'o', 'l', 'd', 'e', 'n'],\n",
       " 'paper': ['p', 'a', 'p', 'e', 'r'],\n",
       " 'wave': ['w', 'a', 'v', 'e'],\n",
       " 'filled': ['f', 'i', 'l', 'l', 'e', 'd'],\n",
       " 'tying': ['t', 'y', 'in', 'g'],\n",
       " 'he': ['h', 'e'],\n",
       " 'passing': ['p', 'a', 's', 's', 'in', 'g'],\n",
       " 'hiking': ['h', 'i', 'k', 'in', 'g'],\n",
       " 'go': ['g', 'o'],\n",
       " 'around': ['a', 'r', 'o', 'u', 'n', 'd'],\n",
       " 'decided': ['d', 'e', 'c', 'i', 'd', 'e', 'd'],\n",
       " 'sandcastle': ['s', 'a', 'n', 'd', 'c', 'a', 's', 't', 'l', 'e'],\n",
       " 'fireworks': ['f', 'i', 'r', 'e', 'w', 'o', 'r', 'k', 's'],\n",
       " 'bright': ['b', 'r', 'i', 'g', 'h', 't'],\n",
       " 'find': ['f', 'in', 'd'],\n",
       " 'wrapped': ['w', 'r', 'a', 'p', 'p', 'e', 'd'],\n",
       " 'behind': ['b', 'e', 'h', 'in', 'd'],\n",
       " 'inside': ['in', 's', 'i', 'd', 'e'],\n",
       " 'she': ['s', 'h', 'e'],\n",
       " 'vibrant': ['v', 'i', 'b', 'r', 'a', 'n', 't'],\n",
       " 'ribbon': ['r', 'i', 'b', 'b', 'o', 'n'],\n",
       " 'other': ['o', 't', 'h', 'e', 'r'],\n",
       " 'cat': ['c', 'a', 't'],\n",
       " 'bakery': ['b', 'a', 'k', 'e', 'r', 'y'],\n",
       " 'finished': ['f', 'in', 'i', 's', 'h', 'e', 'd'],\n",
       " 'her': ['h', 'e', 'r'],\n",
       " 'windowsill': ['w', 'in', 'd', 'o', 'w', 's', 'i', 'l', 'l'],\n",
       " 'day': ['d', 'a', 'y'],\n",
       " 'dark': ['d', 'a', 'r', 'k'],\n",
       " 'dog': ['d', 'o', 'g'],\n",
       " 'brown': ['b', 'r', 'o', 'w', 'n'],\n",
       " 'basking': ['b', 'a', 's', 'k', 'in', 'g'],\n",
       " 'lazy': ['l', 'a', 'z', 'y'],\n",
       " 'street': ['s', 't', 'r', 'e', 'e', 't'],\n",
       " 'in': ['in'],\n",
       " 'was': ['w', 'a', 's'],\n",
       " 'fox': ['f', 'o', 'x'],\n",
       " 'mountains': ['m', 'o', 'u', 'n', 't', 'a', 'in', 's'],\n",
       " 'hard': ['h', 'a', 'r', 'd']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
